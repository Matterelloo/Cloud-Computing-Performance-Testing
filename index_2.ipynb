{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matterelloo/Cloud-Computing-Performance-Testing/blob/main/index_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 Modello"
      ],
      "metadata": {
        "id": "dVmP3K7ownW3"
      },
      "id": "dVmP3K7ownW3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46d2131e",
      "metadata": {
        "id": "46d2131e"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig\n",
        "from peft import get_peft_model\n",
        "from datasets import load_dataset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e291b651",
      "metadata": {
        "id": "e291b651"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(record, tokenizer):\n",
        "  \"\"\"\n",
        "  Tokenize the instruction-answer pairs and returns the prediction labels (the tokens that the model has to predict).\n",
        "  \"\"\"\n",
        "  max_length = 250\n",
        "  conversation = [{\"role\": \"user\", \"content\": f\"{record['question']}\"}, {\"role\": \"assistant\", \"content\": f\"{record['answer']}\"}]\n",
        "  chat = tokenizer.apply_chat_template(conversation, return_tensors='pt', tokenize=False, add_generation_prompt=False)  # chat template used by llama to represent the messages of the assistant and the user\n",
        "  tokens = tokenizer(chat, padding='max_length', return_tensors='pt', add_special_tokens=False, max_length=max_length, truncation=True) # converting the chat template into tokens (used padding and fixed length, useful for the batch processing)\n",
        "  starting_answer_idx = (tokens['input_ids'] == 128007).nonzero()[-1][-1] + 2 # looking where the answer starts (128007 is the token that ends the prompt of the chat)\n",
        "  labels = tokens['input_ids'][0].clone()\n",
        "  labels[:starting_answer_idx] = -100 # setting to -100 the tokens that won't be considered in the computation of the loss\n",
        "  ending_answer_idx = (labels == 128009).nonzero()  # looking where the answer ends in order to identify the padding token (right padding)\n",
        "  if len(ending_answer_idx) != 0:\n",
        "    labels[ending_answer_idx[0]:] = -100  # the padding tokens won't be considered in the computation of the loss\n",
        "\n",
        "  return {\n",
        "    'chat': chat,\n",
        "    'input_ids': tokens['input_ids'][0],\n",
        "    'attention_mask': tokens['attention_mask'][0],\n",
        "    \"labels\": labels,\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e81ca626",
      "metadata": {
        "id": "e81ca626"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b3fb0d9",
      "metadata": {
        "id": "0b3fb0d9"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb069d7c",
      "metadata": {
        "id": "bb069d7c"
      },
      "outputs": [],
      "source": [
        "gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
        "dataset = gsm8k.map(lambda x: preprocess_dataset(x, tokenizer)) # tokenazing the instruction-answer pairs\n",
        "dataset = dataset.remove_columns(['question', 'answer'])  # remove unused columns\n",
        "dataset.set_format(type=\"torch\", columns=dataset['train'].column_names) # used to get tensors instead of lists\n",
        "train_loader = DataLoader(dataset['train'], batch_size=8, shuffle=True) # type: ignore\n",
        "test_loader = DataLoader(dataset['test'], batch_size=8) # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f79fe7ea",
      "metadata": {
        "id": "f79fe7ea"
      },
      "outputs": [],
      "source": [
        "# looking at how many tokens every instruction-answer have in order to understand when to truncate the answer.\n",
        "length = []\n",
        "for item in train_loader:\n",
        "  length.append(len(item['input_ids'][0]))\n",
        "\n",
        "plt.bar(range(len(length)), length)\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Length')\n",
        "plt.title('Barplot of Lengths')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b79e12",
      "metadata": {
        "id": "11b79e12"
      },
      "source": [
        "We can truncate at 250 tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7256577",
      "metadata": {
        "id": "e7256577"
      },
      "outputs": [],
      "source": [
        "device = 'cpu'\n",
        "dtype = torch.float16\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=dtype).to(device)\n",
        "config = LoraConfig(\n",
        "  r=8,\n",
        "  lora_alpha=32,\n",
        "  # target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "  target_modules=[\"q_proj\"], # TODO: to change\n",
        "  lora_dropout=0.05,\n",
        ")\n",
        "lora_model = get_peft_model(model, config)\n",
        "lora_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cb082e1",
      "metadata": {
        "id": "6cb082e1"
      },
      "outputs": [],
      "source": [
        "ignore_index = -100\n",
        "optmizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea9d1280",
      "metadata": {
        "id": "ea9d1280"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "epochs = 2\n",
        "\n",
        "lora_model.train()\n",
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "  for i, batch in enumerate(train_loader):\n",
        "    chat = batch['chat']\n",
        "    input_ids = batch['input_ids'].to(lora_model.device)\n",
        "    attention_mask = batch['attention_mask'].to(lora_model.device)\n",
        "    labels = batch['labels'].to(lora_model.device)\n",
        "    labels = input_ids.clone()\n",
        "    output = lora_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels) # loss directly computed here (CrossEntropyLoss)\n",
        "    optmizer.zero_grad()\n",
        "    output.loss.backward()\n",
        "    optmizer.step()\n",
        "    total_loss += output.loss.item()\n",
        "    print(f\"[Batch {i + 1}/{len(train_loader)}] Loss: {output.loss.item()}\")\n",
        "\n",
        "  losses.append(total_loss / (i + 1))\n",
        "  print(f\"[Epoch {epoch + 1}/{epochs}] Loss: {losses[-1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 Modello"
      ],
      "metadata": {
        "id": "NksiBII2wtHZ"
      },
      "id": "NksiBII2wtHZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "from torch.nn import CrossEntropyLoss\n"
      ],
      "metadata": {
        "id": "kd37da7_w_s7"
      },
      "id": "kd37da7_w_s7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<mask>\"]})\n",
        "MASK_TOKEN_ID = tokenizer.convert_tokens_to_ids(\"<mask>\")\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "gLBslUpc6Z4E"
      },
      "id": "gLBslUpc6Z4E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LlamaWrapperPromptResp(nn.Module):\n",
        "    def __init__(self, model: LlamaForCausalLM):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def make_attention_mask(self, input_ids, prompt_len):\n",
        "        batch_size, seq_size = input_ids.size()\n",
        "        mask = torch.zeros((batch_size, seq_size, seq_size), dtype=torch.bool)\n",
        "        # va be booleani startati a zero tutti i token visbili e poi si ha tipo\n",
        "        # mask[b, seq_size=i, seq_size = j]il token i della sequenza b può guardare il token j\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            p = prompt_len[b] if isinstance(prompt_len, (list, tuple)) else prompt_len\n",
        "            # se prompt_len è una lista usa l'elemento corrispondente (ce va be capito)\n",
        "            # altrimenti è un unico valore per tutto il batch\n",
        "            causal = torch.tril(torch.ones(seq_size, seq_size, dtype=torch.bool))\n",
        "            # sto trill è la triangolare inferiore per guardare solo i token prima\n",
        "            mask[b] = causal # prompt vede solo a sx\n",
        "            mask[b, p:, :] = 1  # risposta vede sx e dx\n",
        "\n",
        "        return mask.to(input_ids.device)\n",
        "\n",
        "    def forward(self, input_ids, prompt_len, **kwargs):\n",
        "        attention_mask = self.make_attention_mask(input_ids, prompt_len)\n",
        "        return self.model(input_ids = input_ids,attention_mask = attention_mask,\n",
        "                          use_cache = False, **kwargs)\n",
        "\n",
        "        # forward del modello usando la attention mask personalizzata\n",
        "        # per prompt e response\n"
      ],
      "metadata": {
        "id": "vGK-4EZ4xLYi"
      },
      "id": "vGK-4EZ4xLYi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_diffusion_step(r0, t, MASK_TOKEN_ID):\n",
        "\n",
        "    batch_size, seq_len = r0.shape\n",
        "    rt = r0.clone()\n",
        "\n",
        "\n",
        "    mask_prob = t.view(-1, 1).expand(batch_size, seq_len)  # broadcast per py.torch\n",
        "    rand = torch.rand_like(rt.float()) #genere stesso shape di numero casuale u< della prob di rt\n",
        "    mask = (rand < mask_prob).long() #coverte booleani in interi\n",
        "\n",
        "    rt = rt * (1 - mask) + MASK_TOKEN_ID * mask\n",
        "    # maschera i token selezionati a random precedentemente\n",
        "    # è su rt perchè è stato clonato da r0\n",
        "    return rt, mask"
      ],
      "metadata": {
        "id": "pWIqQHIo6j4d"
      },
      "id": "pWIqQHIo6j4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ignore_index = -100\n",
        "optmizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "wrapped_model = LlamaWrapperPromptResp(model).to(device)"
      ],
      "metadata": {
        "id": "GXSsuNZ-6xaH"
      },
      "id": "GXSsuNZ-6xaH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for i, batch in enumerate(train_loader): # train_loader con 'input_ids'\n",
        "        r0 = batch['input_ids'].to(device)\n",
        "        batch_size, seq_len = r0.shape\n",
        "\n",
        "        # probabilità di mascheramento casuale per ogni esempio\n",
        "        t = torch.rand(batch_size, device=device)\n",
        "\n",
        "        # Forward diffusion\n",
        "        rt, mask = forward_diffusion_step(r0, t, MASK_TOKEN_ID)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = wrapped_model(input_ids=rt,prompt_len=seq_len//2, labels=r0)\n",
        "\n",
        "        # prompt_len=seq_len//2 -> serve a costruire correttamente l’attention mask\n",
        "        # metà seq prompt, metà seq response\n",
        "\n",
        "        logits = outputs.logits  # [batch_size, seq_len, vocab_size]\n",
        "\n",
        "        # Loss sui token mascherati\n",
        "        labels = r0.clone() #clona\n",
        "        labels[mask == 0] = ignore_index #ignora le label non mascherate\n",
        "        loss_fct = CrossEntropyLoss(ignore_index=ignore_index) # cross\n",
        "        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "        # calcolo effettivo allora da quello che ho capito la cross ha bisogno\n",
        "        # di [N, C] per i logits (N esempi, C classi), quindi:\n",
        "        # [batch_size, seq_len, vocab_size] in in [batch_size * seq_len, vocab_size]\n",
        "        # con --> logits.view(-1, logits.size(-1))\n",
        "        # e poi da .view(-1) trasforma [batch_size, seq_len] in [batch_size * seq_len]\n",
        "\n",
        "        #Casino ma funziona\n",
        "\n",
        "        # Backward e ottimizzazione\n",
        "        optmizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optmizer.step()\n",
        "\n",
        "        #Avarage Loss\n",
        "        total_loss += loss.item()\n",
        "        print(f\"[Batch {i+1}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    losses.append(avg_loss)\n",
        "    print(f\"[Epoch {epoch+1}/{epochs}] Avg Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "a6F6mFST6xWd"
      },
      "id": "a6F6mFST6xWd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parte che worka"
      ],
      "metadata": {
        "id": "VZ72ILTERoel"
      },
      "id": "VZ72ILTERoel"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le funzioni sopra non andavano per una questione di compatibilità della dim quindi ho chiesto a chat"
      ],
      "metadata": {
        "id": "DVKcRpKhRcH8"
      },
      "id": "DVKcRpKhRcH8"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import LlamaForCausalLM\n",
        "\n",
        "\n",
        "class LlamaWrapperPromptResp(nn.Module):\n",
        "    def __init__(self, model: LlamaForCausalLM):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def make_attention_mask(self, input_ids, prompt_len):\n",
        "        batch_size, seq_size = input_ids.size()\n",
        "        mask = torch.zeros((batch_size, seq_size, seq_size), dtype=torch.bool)\n",
        "\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            p = prompt_len[b] if isinstance(prompt_len, (list, tuple)) else prompt_len\n",
        "            causal = torch.tril(torch.ones(seq_size, seq_size, dtype=torch.bool))\n",
        "            mask[b] = causal\n",
        "            mask[b, p:, :] = 1\n",
        "\n",
        "        return mask.to(input_ids.device)\n",
        "\n",
        "    def forward(self, input_ids, prompt_len, **kwargs):\n",
        "        attention_mask = self.make_attention_mask(input_ids, prompt_len)\n",
        "    # aggiungi dimensione \"head\" per compatibilità\n",
        "        attention_mask = attention_mask.unsqueeze(1)\n",
        "        return self.model(input_ids=input_ids,\n",
        "                      attention_mask=attention_mask,\n",
        "                      use_cache=False,\n",
        "                      **kwargs)\n",
        "\n",
        "\n",
        "def forward_diffusion_step(r0, t, MASK_TOKEN_ID):\n",
        "    batch_size, seq_len = r0.shape\n",
        "    rt = r0.clone()\n",
        "\n",
        "    mask_prob = t.view(-1, 1).expand(batch_size, seq_len)\n",
        "    rand = torch.rand_like(rt.float())\n",
        "    mask = (rand < mask_prob).long()\n",
        "\n",
        "    rt = rt * (1 - mask) + MASK_TOKEN_ID * mask\n",
        "    return rt, mask\n",
        "\n",
        "epochs = 5\n",
        "losses = []\n",
        "\n",
        "ignore_index = -100\n",
        "wrapped_model = LlamaWrapperPromptResp(model).to(device)\n",
        "optmizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        r0 = batch['input_ids'].to(device)\n",
        "        batch_size, seq_len = r0.shape\n",
        "\n",
        "\n",
        "        t = torch.rand(batch_size, device=device)\n",
        "\n",
        "\n",
        "        rt, mask = forward_diffusion_step(r0, t, MASK_TOKEN_ID)\n",
        "\n",
        "        # forward pass con la tua attention mask custom\n",
        "        outputs = wrapped_model(input_ids=rt,\n",
        "                                prompt_len=seq_len//2,  # <-- es. metà prompt / metà risposta\n",
        "                                labels=r0)\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "\n",
        "        labels = r0.clone()\n",
        "        labels[mask == 0] = ignore_index\n",
        "        loss_fct = CrossEntropyLoss(ignore_index=ignore_index)\n",
        "        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "\n",
        "        optmizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optmizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        print(f\"[Batch {i+1}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    losses.append(avg_loss)\n",
        "    print(f\"[Epoch {epoch+1}/{epochs}] Avg Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "i4SzWhP6Kk1J"
      },
      "id": "i4SzWhP6Kk1J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prova solo masking prompt-resp"
      ],
      "metadata": {
        "id": "M3Fjt1BJTdKQ"
      },
      "id": "M3Fjt1BJTdKQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avevo finito il limite massimo di lavoro con colab utilizzando gpu e mi mancava di provare se effitavemente si faceva quello che avevo pensato per la roba del masking."
      ],
      "metadata": {
        "id": "RniKoeAORIhz"
      },
      "id": "RniKoeAORIhz"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import LlamaForCausalLM\n",
        "\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "device = \"cpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# per questo esempio possiamo usare input casuali\n",
        "vocab_size = 100\n",
        "seq_len = 10\n",
        "batch_size = 2\n",
        "MASK_TOKEN_ID = 99  # esempio\n",
        "prompt_len = 4      # prompt metà sequenza\n",
        "\n",
        "\n",
        "r0 = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
        "\n",
        "\n",
        "class LlamaWrapperPromptResp(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def make_attention_mask(self, input_ids, prompt_len):\n",
        "        B, S = input_ids.size()\n",
        "        mask = torch.zeros((B, S, S), dtype=torch.bool)\n",
        "        for b in range(B):\n",
        "            p = prompt_len if not isinstance(prompt_len, (list, tuple)) else prompt_len[b]\n",
        "            causal = torch.tril(torch.ones(S, S, dtype=torch.bool))\n",
        "            mask[b] = causal\n",
        "            mask[b, p:, :] = 1\n",
        "        return mask\n",
        "\n",
        "    def forward(self, input_ids, prompt_len, **kwargs):\n",
        "        attention_mask = self.make_attention_mask(input_ids, prompt_len)\n",
        "        return attention_mask\n",
        "\n",
        "\n",
        "def forward_diffusion_step(r0, t, MASK_TOKEN_ID):\n",
        "    B, S = r0.shape\n",
        "    rt = r0.clone()\n",
        "    mask_prob = t.view(-1, 1).expand(B, S)\n",
        "    rand = torch.rand_like(rt.float())\n",
        "    mask = (rand < mask_prob).long()\n",
        "    rt = rt * (1 - mask) + MASK_TOKEN_ID * mask\n",
        "    return rt, mask\n",
        "\n",
        "\n",
        "wrapped_model = LlamaWrapperPromptResp(None)\n",
        "t = torch.rand(batch_size)\n",
        "rt, mask = forward_diffusion_step(r0, t, MASK_TOKEN_ID)\n",
        "attention_mask = wrapped_model(input_ids=rt, prompt_len=prompt_len)\n",
        "\n",
        "\n",
        "for b in range(batch_size):\n",
        "    print(f\"\\nEsempio {b}:\")\n",
        "    print(\"Original input:\", r0[b].tolist())\n",
        "    print(\"Masked input  :\", rt[b].tolist())\n",
        "    print(\"Diffusion mask:\", mask[b].tolist())\n",
        "    print(\"Attention mask:\\n\", attention_mask[b].int())\n"
      ],
      "metadata": {
        "id": "xfVanZPTQhu8"
      },
      "id": "xfVanZPTQhu8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}